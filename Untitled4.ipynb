{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMF8whCJnd7a7cXMobV+ifx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rawan-Hasan/Explainable-AI-for-Legal-Document-Analysis-and-Case-Prediction/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "!pip install PyPDF2\n",
        "!pip install nltk\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download 'punkt_tab' data for sentence tokenization\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as pdf_file:\n",
        "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "        text = \"\"\n",
        "        for page_num in range(len(pdf_reader.pages)):\n",
        "            page = pdf_reader.pages[page_num]\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "\n",
        "# Extract text from the PDF\n",
        "pdf_text = extract_text_from_pdf('/content/2020.lrec-1.155.pdf')\n",
        "\n",
        "# Convert the extracted text to a DataFrame\n",
        "data = pd.DataFrame({'text': [pdf_text]})\n",
        "\n",
        "# Example function for cleaning text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercase text\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize text\n",
        "    return tokens\n",
        "\n",
        "# Apply cleaning function to dataset\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnSaxi_d_3xI",
        "outputId": "8e9bc71c-674d-4abc-91cb-c5bf6234e654"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Example using TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(data['cleaned_text'].apply(lambda x: ' '.join(x)))\n",
        "\n",
        "# Example using embeddings (e.g., BERT from Hugging Face)\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenization and embedding\n",
        "tokens = tokenizer(data['text'].tolist(), return_tensors='pt', padding=True, truncation=True)\n",
        "outputs = model(**tokens)\n",
        "embeddings = outputs.last_hidden_state\n"
      ],
      "metadata": {
        "id": "AFIWGg_lAOeY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BiFeedbackNetwork(nn.Module):\n",
        "    def __init__(self): # Corrected the method name to __init__\n",
        "        super(BiFeedbackNetwork, self).__init__()\n",
        "        # Define network layers (simplified example)\n",
        "        self.fc = nn.Linear(5000, 2)  # Adjust input size according to embedding size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "model = BiFeedbackNetwork()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xpzjcZrUA5_F"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}